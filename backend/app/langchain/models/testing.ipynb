{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "Okay, here's a blog post about the benefits of using LM Studio, aiming for around 600-700 words and suitable for a tech-savvy audience interested in local AI:\n",
      "\n",
      "---\n",
      "\n",
      "## Stop Relying on APIs: Why You Need to Try LM Studio for Local LLM Exploration\n",
      "\n",
      "For months now, we’ve been bombarded with the incredible potential of Large Language Models (LLMs) like ChatGPT. But let's be honest – relying solely on cloud-based APIs can feel limiting. High costs, data privacy concerns, and occasional outages are all valid frustrations.  That's where LM Studio comes in, and it’s rapidly becoming a game-changer for anyone wanting to dive deeper into the world of local AI.\n",
      "\n",
      "**What is LM Studio?**\n",
      "\n",
      "LM Studio is essentially a desktop application that allows you to download, run, and experiment with various open-source Large Language Models *directly on your computer*.  No more API keys, no more worrying about server uptime, just pure, unadulterated LLM power at your fingertips. It’s built around the idea of making these models accessible to everyone – not just those with massive computing resources or deep technical expertise.\n",
      "\n",
      "**Here's Why You Should Be Using LM Studio:**\n",
      "\n",
      "* **Privacy & Security:** This is a *huge* one. When you run an LLM locally, your data stays on your machine.  You don’t have to send sensitive prompts or conversations to a third-party server. This is crucial for anyone dealing with confidential information – legal documents, personal journals, creative writing projects, etc.\n",
      "\n",
      "* **Cost Savings:** API usage can quickly add up, especially if you're experimenting heavily. LM Studio eliminates those per-token costs. Once you download the model, it’s yours to use indefinitely (as long as your hardware can handle it).\n",
      "\n",
      "* **Offline Access:**  Seriously, this is a game changer. Need to brainstorm ideas on a plane? Want to test a prompt without an internet connection? LM Studio lets you do just that. \n",
      "\n",
      "* **Model Variety – A Growing Library:** The beauty of open-source LLMs is the incredible diversity. LM Studio supports a rapidly expanding range of models, including:\n",
      "    * **Llama 2 & Llama 3:** Meta's popular family of models are well supported.\n",
      "    * **Mistral AI Models:** Known for their efficiency and performance.\n",
      "    * **Zephyr:** A fine-tuned version of Mistral focused on helpfulness.\n",
      "    * **And many more!**  The community is constantly adding new models, so the selection grows daily.\n",
      "\n",
      "* **Easy to Use – Seriously!** Don't let the “AI” label intimidate you. LM Studio has a remarkably intuitive interface. You can:\n",
      "    * **Download Models:** Browse and download models directly from within the app.\n",
      "    * **Configure Settings:**  Adjust parameters like context length, temperature (influences randomness), and more to fine-tune the model's behavior.\n",
      "    * **Chat Interface:** Engage in conversations with the LLM using a simple chat window.\n",
      "    * **Extensions:** Integrate with tools like Text-to-Speech and image generation (through extensions).\n",
      "\n",
      "\n",
      "* **Experimentation & Learning:** LM Studio provides an excellent platform to understand how different models respond to various prompts and settings. It's fantastic for learning about LLMs themselves – experimenting is the best way to truly grasp their capabilities.\n",
      "\n",
      "**Is it Right For You?**\n",
      "\n",
      "LM Studio isn’t a replacement for every use case. Running large language models requires decent hardware:\n",
      "\n",
      "* **CPU:** A modern multi-core processor will help, but GPU performance is highly recommended.\n",
      "* **RAM:** 16GB is a good starting point, but 32GB or more will allow you to run larger models comfortably.\n",
      "* **GPU (Recommended):**  A dedicated NVIDIA graphics card with sufficient VRAM (8GB+) drastically improves performance and allows you to run more complex models.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "You can download LM Studio for free from [https://lmstudio.ai/](https://lmstudio.ai/). It's available for Windows, macOS, and Linux.  Give it a try – you might be surprised at how powerful and accessible local LLMs can be!\n",
      "\n",
      "**Resources & Community:**\n",
      "\n",
      "* **LM Studio Website:** [https://lmstudio.ai/](https://lmstudio.ai/)\n",
      "* **Discord Server:** [https://discord.com/invite/c9z6eWq5j8](https://discord.com/invite/c9z6eWq5j8) (Great for support and finding new models!)\n",
      "\n",
      "---\n",
      "\n",
      "**Would you like me to:**\n",
      "\n",
      "*   Expand on a specific section?\n",
      "*   Tailor the post to a particular audience (e.g., beginners, developers)?\n",
      "*   Add screenshots or links to example prompts?\n",
      "\n",
      "Final full response:\n",
      " Okay, here's a blog post about the benefits of using LM Studio, aiming for around 600-700 words and suitable for a tech-savvy audience interested in local AI:\n",
      "\n",
      "---\n",
      "\n",
      "## Stop Relying on APIs: Why You Need to Try LM Studio for Local LLM Exploration\n",
      "\n",
      "For months now, we’ve been bombarded with the incredible potential of Large Language Models (LLMs) like ChatGPT. But let's be honest – relying solely on cloud-based APIs can feel limiting. High costs, data privacy concerns, and occasional outages are all valid frustrations.  That's where LM Studio comes in, and it’s rapidly becoming a game-changer for anyone wanting to dive deeper into the world of local AI.\n",
      "\n",
      "**What is LM Studio?**\n",
      "\n",
      "LM Studio is essentially a desktop application that allows you to download, run, and experiment with various open-source Large Language Models *directly on your computer*.  No more API keys, no more worrying about server uptime, just pure, unadulterated LLM power at your fingertips. It’s built around the idea of making these models accessible to everyone – not just those with massive computing resources or deep technical expertise.\n",
      "\n",
      "**Here's Why You Should Be Using LM Studio:**\n",
      "\n",
      "* **Privacy & Security:** This is a *huge* one. When you run an LLM locally, your data stays on your machine.  You don’t have to send sensitive prompts or conversations to a third-party server. This is crucial for anyone dealing with confidential information – legal documents, personal journals, creative writing projects, etc.\n",
      "\n",
      "* **Cost Savings:** API usage can quickly add up, especially if you're experimenting heavily. LM Studio eliminates those per-token costs. Once you download the model, it’s yours to use indefinitely (as long as your hardware can handle it).\n",
      "\n",
      "* **Offline Access:**  Seriously, this is a game changer. Need to brainstorm ideas on a plane? Want to test a prompt without an internet connection? LM Studio lets you do just that. \n",
      "\n",
      "* **Model Variety – A Growing Library:** The beauty of open-source LLMs is the incredible diversity. LM Studio supports a rapidly expanding range of models, including:\n",
      "    * **Llama 2 & Llama 3:** Meta's popular family of models are well supported.\n",
      "    * **Mistral AI Models:** Known for their efficiency and performance.\n",
      "    * **Zephyr:** A fine-tuned version of Mistral focused on helpfulness.\n",
      "    * **And many more!**  The community is constantly adding new models, so the selection grows daily.\n",
      "\n",
      "* **Easy to Use – Seriously!** Don't let the “AI” label intimidate you. LM Studio has a remarkably intuitive interface. You can:\n",
      "    * **Download Models:** Browse and download models directly from within the app.\n",
      "    * **Configure Settings:**  Adjust parameters like context length, temperature (influences randomness), and more to fine-tune the model's behavior.\n",
      "    * **Chat Interface:** Engage in conversations with the LLM using a simple chat window.\n",
      "    * **Extensions:** Integrate with tools like Text-to-Speech and image generation (through extensions).\n",
      "\n",
      "\n",
      "* **Experimentation & Learning:** LM Studio provides an excellent platform to understand how different models respond to various prompts and settings. It's fantastic for learning about LLMs themselves – experimenting is the best way to truly grasp their capabilities.\n",
      "\n",
      "**Is it Right For You?**\n",
      "\n",
      "LM Studio isn’t a replacement for every use case. Running large language models requires decent hardware:\n",
      "\n",
      "* **CPU:** A modern multi-core processor will help, but GPU performance is highly recommended.\n",
      "* **RAM:** 16GB is a good starting point, but 32GB or more will allow you to run larger models comfortably.\n",
      "* **GPU (Recommended):**  A dedicated NVIDIA graphics card with sufficient VRAM (8GB+) drastically improves performance and allows you to run more complex models.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "You can download LM Studio for free from [https://lmstudio.ai/](https://lmstudio.ai/). It's available for Windows, macOS, and Linux.  Give it a try – you might be surprised at how powerful and accessible local LLMs can be!\n",
      "\n",
      "**Resources & Community:**\n",
      "\n",
      "* **LM Studio Website:** [https://lmstudio.ai/](https://lmstudio.ai/)\n",
      "* **Discord Server:** [https://discord.com/invite/c9z6eWq5j8](https://discord.com/invite/c9z6eWq5j8) (Great for support and finding new models!)\n",
      "\n",
      "---\n",
      "\n",
      "**Would you like me to:**\n",
      "\n",
      "*   Expand on a specific section?\n",
      "*   Tailor the post to a particular audience (e.g., beginners, developers)?\n",
      "*   Add screenshots or links to example prompts?\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List, Iterator, Any\n",
    "\n",
    "import json\n",
    "import httpx\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk, BaseMessage, HumanMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.callbacks import CallbackManagerForChainRun\n",
    "from pydantic import Field\n",
    "\n",
    "# from app.global_settings import API_BASE_URL\n",
    "API_BASE_URL = \"http://127.0.0.1:1234\"\n",
    "\n",
    "\n",
    "class LmStudioChatModel(BaseChatModel):\n",
    "    model_name: str = Field(alias=\"model\")\n",
    "    temperature: Optional[float] = None\n",
    "    max_tokens: Optional[int] = None\n",
    "    timeout: Optional[int] = None\n",
    "    stop: Optional[List[str]] = None\n",
    "    max_retries: int = 2\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        formatted_messages = [\n",
    "            {\"role\": \"user\", \"content\": message.content} for message in messages\n",
    "        ]\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": formatted_messages,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"timeout\": self.timeout,\n",
    "            \"stop\": self.stop,\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = httpx.post(\n",
    "                f\"{API_BASE_URL}/v1/chat/completions\",\n",
    "                json=payload,\n",
    "                headers=headers,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return ChatResult(generations=[ChatGeneration(message=AIMessage(content=content))])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise e\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        formatted_messages = [\n",
    "            {\"role\": \"user\", \"content\": message.content} for message in messages\n",
    "        ]\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": formatted_messages,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"timeout\": self.timeout,\n",
    "            \"stop\": self.stop,\n",
    "            \"stream\": True,\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with httpx.stream(\n",
    "                \"POST\",\n",
    "                f\"{API_BASE_URL}/v1/chat/completions\",\n",
    "                json=payload,\n",
    "                headers=headers,\n",
    "                timeout=self.timeout or 30,\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                buffer = \"\"\n",
    "                for line in response.iter_lines():\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        data = line[len(\"data: \") :].strip()\n",
    "                        if data == \"[DONE]\":\n",
    "                            break\n",
    "                        try:\n",
    "                            chunk = json.loads(data)\n",
    "                            delta = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "                            if delta:\n",
    "                                yield ChatGenerationChunk(\n",
    "                                    message=AIMessageChunk(content=delta)\n",
    "                                )\n",
    "                                if run_manager:\n",
    "                                    run_manager.on_llm_new_token(delta)\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Streaming error: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def invoke(self, input: str, **kwargs: Any) -> AIMessage:\n",
    "        print(\"Streaming response:\")\n",
    "        messages = [HumanMessage(content=input)]\n",
    "        full_content = \"\"\n",
    "\n",
    "        for chunk in self._stream(messages):\n",
    "            token = chunk.message.content\n",
    "            print(token, end=\"\", flush=True)\n",
    "            full_content += token\n",
    "\n",
    "        print()  # Newline after stream ends\n",
    "        return AIMessage(content=full_content)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"lmstudio\"\n",
    "\n",
    "\n",
    "def get_chat_model(\n",
    "    temperature: float = 0.7,\n",
    "    model_name: str = \"gemma-3-4b-it\",\n",
    "    api_base_url: Optional[str] = None,\n",
    ") -> LmStudioChatModel:\n",
    "    \"\"\"\n",
    "    Get a configured ChatOpenAI instance.\n",
    "\n",
    "    Args:\n",
    "        temperature: Controls randomness in the output (0.0 to 1.0)\n",
    "        model_name: The name of the model to use\n",
    "        api_base_url: Optional custom API base URL\n",
    "\n",
    "    Returns:\n",
    "        ChatOpenAI: Configured chat model instance\n",
    "    \"\"\"\n",
    "    return LmStudioChatModel(\n",
    "        temperature=temperature,\n",
    "        model_name=model_name,\n",
    "        openai_api_base=api_base_url or API_BASE_URL,\n",
    "    )\n",
    "\n",
    "\n",
    "model = LmStudioChatModel(\n",
    "    temperature=0.7,\n",
    "    model_name=\"gemma-3-4b-it\",\n",
    "    api_base_url=API_BASE_URL,\n",
    ")\n",
    "\n",
    "response = model.invoke(\"Write a blog post about the benefits of using LM Studio.\")\n",
    "print(\"\\nFinal full response:\\n\", response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
